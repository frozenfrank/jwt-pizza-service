# Incident: 2024-12-01 10:37:50

## Summary

Between the hours of 10:31 and 10:57 am MST on Sunday, Dec 12th, 0 users encountered errors when ordering pizzas through the JWT pizza service. The errors were triggered by an outage of the external JWT Pizza Factory sometime between 10:31 and 10:37. The failure was initiated because of a voluntary request for chaos testing of the system.

JWT Pizza Service is dependent on JWT Pizza Factory to sign and acknowledge pizza orders. The outage was detected by Grafana alerts monitoring a non-zero rate of pizza failures. The team started working on the event by reviewing error log data on the JWT Pizza Service Dashboard to locate the resolution URL which ended the chaos. Because the automated monitoring alerted of the errors, this critical incident affected 0% of users.

There was no further impact of this incident. Logs indicate that only our health checks were using the system during the time of the outage.

## Detection

This incident was detected when the Pizza Factory Failure alarm was triggered and the JWT Pizza OnCall time (James Finlinson) was paged.

The event may have been detected sooner with more frequent health checks. The interval of 10 minutes contributed to a total potential down time of 20 minutes. However, we have decided that the 10 minute interval provided sufficient notice of the behavior. No configuration changes will be made.

After receiving the alert, the response could have been quickened by using more convenient tools. Accessing the dashboard on a mobile phone presented UI issues scrolling to the end of the log message to copy the report URL. Creativity had to be employed to open the logs in a different window which allowed the string to be copied and sent to resolve the situation. A new policy that responding individual access the dashboard from fully capable devices such as laptops and computers will reduce this time in the future.

### Source Metrics and Logs

Grafana: [JWT Pizza Service dashboard](https://frozenfrank.grafana.net/d/fe59vb10qlmo0c/pizza-dashboard?orgId=1&from=2024-12-01T17:24:56.110Z&to=2024-12-01T18:12:20.813Z&timezone=browser&var-METRICS_SOURCE=source%7C%3D%7Cjwt-pizza-service&var-LOGS_SOURCE=component%7C%3D%7Cjwt-pizza-service)

PDF: [Grafana JWT Pizza Service dashboard over the incident time period](./incident-2024-12-01-1-dashboard.pdf)

The Orders Placed panel which generated the alarm.

![Orders Placed Metric Panel](./panel-orders-placed.png)

Four total [error logs](./error-logs.txt) were recorded during the incident. The first error log is included as a sample.

```txt
1733075197946	{"authorized":true,"path":"/api/order","ip":"::ffff:172.31.23.204","method":"POST","latency":186,"statusCode":500,"sessionId":"eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpZCI6OCwibmFtZSI6IlRlc3RpbmciLCJlbWFpbCI6InRlc3RpbmdAd2hlYXRoYXJ2ZXN0LmxsYyIsInJvbGVzIjpbeyJyb2xlIjoiZGluZXIifV0sInRva2VuX3NhbHQiOjAuNDg0NjIxODY1ODU2MjQzMSwidGltZXN0YW1wIjoxNzMzMDc1MTk3NTI3LCJpYXQiOjE3MzMwNzUxOTd9.lFFYgjaLMFvKafRjq5HtZQJyTXUkOB9MAeuGIrOjVIA","reqBody":"{\"items\":[{\"menuId\":2,\"description\":\"Pepperoni\",\"price\":0.0042},{\"menuId\":4,\"description\":\"Crusty\",\"price\":0.0028},{\"menuId\":1,\"description\":\"Veggie\",\"price\":0.0038},{\"menuId\":1,\"description\":\"Veggie\",\"price\":0.0038},{\"menuId\":1,\"description\":\"Veggie\",\"price\":0.0038},{\"menuId\":7,\"description\":\"Pepperoni\",\"price\":0.0042},{\"menuId\":7,\"description\":\"Pepperoni\",\"price\":0.0042},{\"menuId\":10,\"description\":\"Charred Leopard\",\"price\":0.0099}],\"storeId\":\"1\",\"franchiseId\":1}","resBody":"{\"message\":\"Failed to fulfill order at factory\",\"reportUrl\":\"https://cs329.cs.byu.edu/api/report?apiKey=37e56826b2da4693b5a5441d7fdbe11f&fixCode=15d279b59e3f493388a4e20241ce21fa\"}"}
```

## Impact

For 20-27 minutes between 10:31 MST and 10:57 MST on 12/01/24, our users experienced this outage.

This incident affected 0 customers (100% OF JWT Pizza USERS), who experienced errors if they attempted to order pizzas.

The alarm woke 1 developer from his sleep to respond to the incident.

No support tickets or social media posts were submitted.

## Timeline

All times are UTC.

- _17:27_ - Order and Verify Pizza period checks begin
- _17:30_ - Order and Verify Pizza period checks all complete successfully
- _17:36_ - Order and Verify Pizza period checks fail
- _17:36:50_ - Pizza Factory Failure alarm enters pending state (#7)
- _17:37:50_ - Pizza Factory Failure alarm begins firing (#7)
- _17:38:27_ - Push notification delivered to developer (#7)
- _17:39:37_ - Important push notification delivered to developer (#7)
- _17:40:49_ - SMS notification delivered to developer (#7)
- _17:42:17_ - Developer acknowledges receipt of alarm (#7)
- _17:47_ - Order and Verify Pizza period checks fail
- _17:46:50_ - Pizza Factory Failure alarm enters pending state (#8)
- _17:47:50_ - Pizza Factory Failure alarm begins firing (#8)
- _17:48:28_ - Push notification delivered to developer (#8)
- _17:48:45_ - Developer acknowledges receipt of alarm (#8)
- _17:46-17:53_ - Developer copies and submits the resolution URL
- _17:57_ - Order and Verify Pizza period checks succeed. Impact is resolved.

## Response

After receiving a page at {17:38 UTC}, James Finlinson came online at 17:42 UTC in Grafana OnCall IRM.

The engineer immediately began using a mobile device to view the logs and locate the resolution URL.

Because the alerting window was short, the alarm auto-resolved itself and fired again when the next round of health checks came through.

# Root cause

This was caused by voluntary submission to chaos testing to test the responsiveness of the alerting system. The situation was resolved in a reasonable amount of time, although the time could have been reduced with more frequent health checks and more capable devices during the response.

## Resolution

Describe how the service was restored and the incident was deemed over. Detail how the service was successfully restored and you knew how what steps you needed to take to recovery.

Depending on the scenario, consider these questions: How could you improve time to mitigation? How could you have cut that time by half?

**EXAMPLE**:

We used a three-pronged approach to the recovery of the system:

{DESCRIBE THE ACTION THAT MITIGATED THE ISSUE, WHY IT WAS TAKEN, AND THE OUTCOME}

**Example**:
By Increasing the size of the BuildEng EC3 ASG to increase the number of nodes available to support the workload and reduce the likelihood of scheduling on oversubscribed nodes

Disabled the Escalator autoscaler to prevent the cluster from aggressively scaling-down
Reverting the Build Engineering scheduler to the previous version.

# Prevention

Now that you know the root cause, can you look back and see any other incidents that could have the same root cause? If yes, note what mitigation was attempted in those incidents and ask why this incident occurred again.

**EXAMPLE**:

This same root cause resulted in incidents HOT-13432, HOT-14932 and HOT-19452.

# Action items

Describe the corrective action ordered to prevent this class of incident in the future. Note who is responsible and when they have to complete the work and where that work is being tracked.

**EXAMPLE**:

1. Manual auto-scaling rate limit put in place temporarily to limit failures
1. Unit test and re-introduction of job rate limiting
1. Introduction of a secondary mechanism to collect distributed rate information across cluster to guide scaling effects
